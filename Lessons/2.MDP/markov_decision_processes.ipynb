{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "\n",
    "\n",
    "The lessons introduce the notion of Markov Decision Process (MDP) in three steps. This notion is import as a lot of RL problems can be formalised as MDPs\n",
    "\n",
    "## Student Markov Chain\n",
    "\n",
    "\n",
    "A state $S_t$ is Markov if and only if:\n",
    "\n",
    "$\\mathbb P [S_{t+1}|S_{t}]=\\mathbb P [S_{t+1}|S_{1},...,S_{t}]$\n",
    "\n",
    "\n",
    "A Markov Process is a tuple $(\\mathcal S ,\\mathcal P )$:\n",
    "* $ \\mathcal S $ a finite set of states\n",
    "* $ \\mathcal P $ a state transition probability matrix : $\\mathcal P _{ss'}= \\mathbb P [S_{t+1}=s'|S_{t}=s]$\n",
    "\n",
    "\n",
    "\n",
    "A running example of the course on which I did some implementations is the student Markov chain\n",
    "<p align=\"center\">\n",
    "\t<img src=\"./Images/MP.png\">\n",
    "</p>\n",
    "I implemented this markov chain in python be able to able probe the markov process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class StudentMarkovChain():\n",
    "    \"\"\"\n",
    "    This class models the Markov process described in the lesson\n",
    "    everything is hard coded, this class is not meant to be general\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #transition probabilities of each state\n",
    "        self.transition = np.array([[0, 0.5 , 0 , 0 , 0 , 0.5 , 0 ],\n",
    "                                    [0 , 0 , 0.8 , 0 , 0 , 0 , 0.2],\n",
    "                                    [0 , 0 , 0 , 0.6 , 0.4 , 0 , 0],\n",
    "                                    [0 , 0 , 0 , 0 , 0 , 0 , 1],\n",
    "                                    [0.2 , 0.4 , 0.4 , 0 , 0 , 0 , 0],\n",
    "                                    [0.1 , 0 , 0 , 0 , 0 , 0.9 , 0],\n",
    "                                    [0 , 0 , 0 , 0 , 0 , 0 , 1]\n",
    "                                    ])\n",
    "        #name of states\n",
    "        self.titles=[\"C1\",\"C2\",\"C3\",\"Pass\",\"Pub\",\"FB\",\"Sleep\"]\n",
    "        #first state\n",
    "        self.state=0\n",
    "        #the class will keep the history\n",
    "        self.history = [self.titles[self.state]]\n",
    "\n",
    "    #function to change state in the markov process\n",
    "    def step(self):\n",
    "        #Next state is picked following the probabilities of the transition matrix \n",
    "        self.state = np.random.choice(range(7),p=self.transition[self.state])\n",
    "        self.history.append(self.titles[self.state])\n",
    "        #if the state is  final\n",
    "        if self.state != 6:\n",
    "            #we return a state a bool telling if it is finished\n",
    "            return self.state,False\n",
    "        else:\n",
    "            return self.state,True\n",
    "    #function to restart\n",
    "    def reboot(self):\n",
    "        self.state = 0\n",
    "        self.history = [self.titles[self.state]]\n",
    "\n",
    "\n",
    "#function to run the markov chain\n",
    "def main_markov():\n",
    "    finished = False\n",
    "    smc = StudentMarkovChain()\n",
    "    while not finished:\n",
    "        _,finished = smc.step()\n",
    "    print(smc.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C1', 'C2', 'C3', 'Pub', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'C2', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'C2', 'C3', 'Pub', 'C3', 'Pub', 'C2', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'C2', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'FB', 'FB', 'C1', 'FB', 'FB', 'FB', 'C1', 'FB', 'FB', 'FB', 'C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'FB', 'FB', 'FB', 'C1', 'C2', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'C2', 'C3', 'Pub', 'C1', 'C2', 'C3', 'Pub', 'C2', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'C2', 'C3', 'Pub', 'C2', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'C2', 'C3', 'Pub', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'C2', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'C2', 'Sleep']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    main_markov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see after probing the markov process that students are enclined to pass an awful amount of time on facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Reward Process\n",
    "\n",
    "A Markov Reward Process is a tuple $(\\mathcal S ,\\mathcal P,\\mathcal R ,\\gamma)$ where:\n",
    "* $ \\mathcal S $ a finite set of states\n",
    "* $ \\mathcal P $ a state transition probability matrix : $\\mathcal P _{ss'}= \\mathbb P [S_{t+1}=s'|S_{t}=s]$\n",
    "* $ \\mathcal R $ is a reward function, $ \\mathcal R _s = \\mathbb E [R_{t+1}|S_t = s] $ \n",
    "* $ \\gamma $ is a discount factor, $\\gamma \\in [0,1]$\n",
    "\n",
    "Here we present the Student reward process:\n",
    "\n",
    "<p align=\"center\">\n",
    "\t<img src=\"./Images/MRP.png\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StudentMarkovRewardProcess(StudentMarkovChain):\n",
    "    \"\"\"\n",
    "    Class to add rewards to the student markov chain\n",
    "    it is inheriting the transition probabilities and the names from the markov chain\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor \n",
    "        \"\"\"\n",
    "\n",
    "        StudentMarkovChain.__init__(self)\n",
    "        # we are adding here the rewards of the different states\n",
    "        self.rewards=[-2,-2,-2,10,1,-1,0]\n",
    "        #and the shape of the history includes the rewards \n",
    "        self.history[-1]=(self.history[-1],self.rewards[self.state])\n",
    "\n",
    "    # change the step function of the markov chain to add the rewards    \n",
    "    def step(self):\n",
    "        state,finished = StudentMarkovChain.step(self)\n",
    "        reward = self.rewards[state]\n",
    "        self.history[-1]=(self.history[-1],reward)\n",
    "        return self.state,finished,reward\n",
    "\n",
    "    #function to restart\n",
    "    def reboot(self):\n",
    "        self.state = 0\n",
    "        self.history = [(self.titles[self.state],self.rewards[self.state])]\n",
    "        \n",
    "#function to run the markov chain\n",
    "def main_markov_reward():\n",
    "    finished = False\n",
    "    srp = StudentMarkovRewardProcess()\n",
    "    while not finished:\n",
    "        _,finished,_ = srp.step()\n",
    "    print(srp.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('FB', -1), ('C1', -2), ('C2', -2), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "[('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('C2', -2), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "[('C1', -2), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('C2', -2), ('Sleep', 0)]\n",
      "[('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('C2', -2), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "[('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('C2', -2), ('C3', -2), ('Pub', 1), ('C2', -2), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "[('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('C2', -2), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "[('C1', -2), ('C2', -2), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "[('C1', -2), ('C2', -2), ('Sleep', 0)]\n",
      "[('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('C1', -2), ('C2', -2), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "[('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('C2', -2), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n"
     ]
    }
   ],
   "source": [
    "#the history now has a reward attached to it\n",
    "for i in range(10):\n",
    "    main_markov_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A central notion in reinforcement is the return $G_{t}$ that the agent can expect from now on to the end of the episode:\n",
    "$$ G_{t}= R_{t+1}+\\gamma R_{t+2}+ ... = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}$$\n",
    "\n",
    "The state value is of a MRP is the expected return starting from state $s$:\n",
    "$$ v(s) = \\mathbb E [G_{t}|S_t = s]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_return_markov_reward(verbose=True):\n",
    "    gamma =0.9\n",
    "    finished = False\n",
    "    srp = StudentMarkovRewardProcess()\n",
    "    while not finished:\n",
    "        _,finished,_ = srp.step()\n",
    "    \n",
    "    rtn = sum([gamma**j*reward for j,(_,reward) in enumerate(srp.history)])\n",
    "    if verbose:\n",
    "        print(srp.history)\n",
    "        print(\"Return of state 1: \",rtn)\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C1', -2), ('C2', -2), ('C3', -2), ('Pub', 1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('C2', -2), ('C3', -2), ('Pub', 1), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "Return of state 1:  -11.7031921009334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-11.7031921009334"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_return_markov_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we average the return obtained on the state 1 we will obtain the state value of state 1. it would just mean making a loop on the previous function and average the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.09440703804801"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([compute_return_markov_reward(verbose=False) for _ in range(1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a value of -5.09 as the value of state 1 (the real value being -5 as we are about to see)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman equation for MRP\n",
    "The bellman equation is the following one for MRP:\n",
    "$$\n",
    "v(s) = \\mathbb E [R_{t+1} + \\gamma v(S_{t+1})|S_{t}=s]\n",
    "$$\n",
    "As demonstrated in the lesson, using the bellman equation, we can derive an exact formula for the state values:\n",
    "\n",
    "\n",
    " $$\\begin{align}\n",
    " \\begin{bmatrix}\n",
    "           v(1) \\\\\n",
    "           \\vdots \\\\\n",
    "           v(n)\n",
    "         \\end{bmatrix}\n",
    "         = \n",
    "         \\begin{bmatrix}\n",
    "           \\mathcal R _1 \\\\\n",
    "           \\vdots \\\\\n",
    "            \\mathcal R _n\n",
    "         \\end{bmatrix} + \\gamma \n",
    "         \\begin{bmatrix}\n",
    "           \\mathcal P _{11} & \\cdots & \\mathcal P _{1n} \\\\\n",
    "           \\vdots & \\cdots& \\vdots\\\\\n",
    "            \\mathcal P _{n1} & \\cdots & \\mathcal P _{nn}\n",
    "         \\end{bmatrix}\n",
    "         \\begin{bmatrix}\n",
    "           v(1) \\\\\n",
    "           \\vdots \\\\\n",
    "           v(n)\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}\n",
    "  $$\n",
    "  \n",
    "  The solution of this system of equation (not doable for complex MRP) is given by:\n",
    "  \n",
    "  $$\n",
    "  v = (I -\\gamma \\mathcal P)^{-1}\\mathcal R\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C1': -5.0127289100145225, 'C2': 0.9426552976939075, 'C3': 4.087021246797094, 'Pass': 10.0, 'Pub': 1.9083923522141464, 'FB': -7.637608431059512, 'Sleep': 0.0}\n"
     ]
    }
   ],
   "source": [
    "#computing the direct solution of the MDP \n",
    "#if gamma = 1 this computation is not doable\n",
    "def direct_solution():\n",
    "    smc = StudentMarkovRewardProcess()\n",
    "    gamma = 0.9\n",
    "    rev = np.dot(np.linalg.inv(np.eye(7)- gamma*smc.transition),smc.rewards)\n",
    "    print(dict(zip(smc.titles,rev)))\n",
    "\n",
    "direct_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find that the value of state 1 is -5 just like we computed just above, this computation has to be approximated in the case of a large MRP using\n",
    "* Dynamic programming\n",
    "* Monte Carlo\n",
    "* TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov decision process\n",
    "\n",
    "A Markov Decision Process is a tuple $(\\mathcal S ,\\mathcal A, \\mathcal P,\\mathcal R ,\\gamma)$ (it is basically a reward process with decisions) where:\n",
    "* $ \\mathcal S $ a finite set of states\n",
    "* $\\mathcal A$ is a finite set of actions \n",
    "* $ \\mathcal P $ a state transition probability matrix : $\\mathcal P _{ss'}^{a}= \\mathbb P [S_{t+1}=s'|S_{t}=s,A_t = a]$\n",
    "* $ \\mathcal R $ is a reward function, $ \\mathcal R _s^a = \\mathbb E [R_{t+1}|S_t = s,A_t = a] $ \n",
    "* $ \\gamma $ is a discount factor, $\\gamma \\in [0,1]$\n",
    "\n",
    "<p align=\"center\">\n",
    "\t<img src=\"./Images/MDP.png\">\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is a implementation of the student MDP that we will use in the next lessons\n",
    "class MarkovStudentDecisionProcess():\n",
    "    \"\"\"\n",
    "    Markov Decision Process of the Student graph presented in the lesson\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "\n",
    "        #less states than before \n",
    "        self.titles=[\"C1\",\"C2\",\"C3\",\"FB\",\"Sleep\"]\n",
    "        self.state=0\n",
    "        #transition probabilites depend on the action taken by the agent\n",
    "        # first line are the values for state one (c1), second line are the values for state 2 (c2),...\n",
    "        #the transition matrix does have an extra dimension with the actions\n",
    "        self.transition = [{\"Study\":[0,1,0,0,0],'Facebook':[0,0,0,1,0]},\n",
    "                           {'Study':[0,0,1,0,0],'Sleep':[0,0,0,0,1]},\n",
    "                           {'Study':[0,0,0,0,1],'Pub':[0.2,0.4,0.4,0,0]},\n",
    "                           {'Facebook':[0,0,0,1,0],\"Quit\":[1,0,0,0,0]},\n",
    "                           {\"Sleep\":[0,0,0,0,1]}\n",
    "                            ]\n",
    "        #the reward depends on the state and action taken by the agent\n",
    "        self.reward=[{\"Study\":-2,'Facebook':-1},\n",
    "                   {'Study':-2,'Sleep':0},\n",
    "                   {'Study':10,'Pub':1},\n",
    "                   {'Facebook':-1,\"Quit\":0},\n",
    "                   {\"Sleep\":0}]\n",
    "\n",
    "\n",
    "    # step depends now of the action taken by the agent    \n",
    "    def step(self,action):\n",
    "        reward = self.reward[self.state][action]\n",
    "        self.state = np.random.choice(range(5),p=self.transition[self.state][action])\n",
    "        \n",
    "        \n",
    "        finished=(action == \"Sleep\")\n",
    "        return self.state,reward,finished\n",
    "        #no history stored here the history will be stored by the agent\n",
    "    def reboot(self):\n",
    "        self.state = 0\n",
    "\n",
    "\n",
    "class AgentRandom():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Random agent has no will it just pick random actions\n",
    "        \"\"\"\n",
    "        self.actions = [[\"Study\",'Facebook'],\n",
    "                       ['Study','Sleep'],\n",
    "                       ['Study','Pub'],\n",
    "                       ['Facebook',\"Quit\"],\n",
    "                       [\"Sleep\"]]\n",
    "\n",
    "    #selecting action randomly                   \n",
    "    def select_action(self,state):\n",
    "        return np.random.choice(self.actions[state])\n",
    "\n",
    "    #virtual update\n",
    "    def update(self,**kwargs):\n",
    "        pass\n",
    "    \n",
    "# simple function to run the episode with a random agent\n",
    "def run_episode():\n",
    "    finished=False\n",
    "    environement = MarkovStudentDecisionProcess()\n",
    "    agent = AgentRandom()\n",
    "    history = []\n",
    "    while not finished:\n",
    "        state = environement.state\n",
    "        action = agent.select_action(state)\n",
    "        _,reward,finished = environement.step(action)\n",
    "        history.append((environement.titles[state],action,reward))\n",
    "    print(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C1', 'Study', -2), ('C2', 'Sleep', 0)]\n"
     ]
    }
   ],
   "source": [
    "# running a sampled episode\n",
    "run_episode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you'll note that the history now includes a state, an action and a reward\n",
    "\n",
    "I will not talk here of Partially observable Markov decision process or ergoticity sorry for those wanting it :-(\n",
    "Next lesson is about dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
