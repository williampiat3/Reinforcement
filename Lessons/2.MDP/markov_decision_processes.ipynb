{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "\n",
    "\n",
    "The lessons introduce the notion of Markov Decision Process (MDP) in three steps. This notion is import as a lot of RL problems can be formalised as MDPs\n",
    "\n",
    "## Student Markov Chain\n",
    "\n",
    "\n",
    "A state $S_t$ is Markov if and only if:\n",
    "\n",
    "$\\mathbb P [S_{t+1}|S_{t}]=\\mathbb P [S_{t+1}|S_{1},...,S_{t}]$\n",
    "\n",
    "\n",
    "A Markov Process is a tuple $(\\mathcal S ,\\mathcal P )$:\n",
    "* $ \\mathcal S $ a finite set of states\n",
    "* $ \\mathcal P $ a state transition probability matrix : $\\mathcal P _{ss'}= \\mathbb P [S_{t+1}=s'|S_{t}=s]$\n",
    "\n",
    "\n",
    "\n",
    "A running example of the course on which I did some implementations is the student Markov chain\n",
    "<p align=\"center\">\n",
    "\t<img src=\"./Images/MP.png\">\n",
    "</p>\n",
    "I implemented this markov chain in python be able to able probe the markov process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class StudentMarkovChain():\n",
    "    \"\"\"\n",
    "    This class models the Markov process described in the lesson\n",
    "    everything is hard coded, this class is not meant to be general\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #transition probabilities of each state\n",
    "        self.transition = np.array([[0, 0.5 , 0 , 0 , 0 , 0.5 , 0 ],\n",
    "                                    [0 , 0 , 0.8 , 0 , 0 , 0 , 0.2],\n",
    "                                    [0 , 0 , 0 , 0.6 , 0.4 , 0 , 0],\n",
    "                                    [0 , 0 , 0 , 0 , 0 , 0 , 1],\n",
    "                                    [0.2 , 0.4 , 0.4 , 0 , 0 , 0 , 0],\n",
    "                                    [0.1 , 0 , 0 , 0 , 0 , 0.9 , 0],\n",
    "                                    [0 , 0 , 0 , 0 , 0 , 0 , 1]\n",
    "                                    ])\n",
    "        #name of states\n",
    "        self.titles=[\"C1\",\"C2\",\"C3\",\"Pass\",\"Pub\",\"FB\",\"Sleep\"]\n",
    "        #first state\n",
    "        self.state=0\n",
    "        #the class will keep the history\n",
    "        self.history = [self.titles[self.state]]\n",
    "\n",
    "    #function to change state in the markov process\n",
    "    def step(self):\n",
    "        #Next state is picked following the probabilities of the transition matrix \n",
    "        self.state = np.random.choice(range(7),p=self.transition[self.state])\n",
    "        self.history.append(self.titles[self.state])\n",
    "        #if the state is  final\n",
    "        if self.state != 6:\n",
    "            #we return a state a bool telling if it is finished\n",
    "            return self.state,False\n",
    "        else:\n",
    "            return self.state,True\n",
    "    #function to restart\n",
    "    def reboot(self):\n",
    "        self.state = 0\n",
    "        self.history = [self.titles[self.state]]\n",
    "\n",
    "\n",
    "#function to run the markov chain\n",
    "def main_markov():\n",
    "    finished = False\n",
    "    smc = StudentMarkovChain()\n",
    "    while not finished:\n",
    "        _,finished = smc.step()\n",
    "    print(smc.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C1', 'C2', 'C3', 'Pub', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'C2', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'FB', 'C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'C2', 'C3', 'Pub', 'C2', 'Sleep']\n",
      "['C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'C2', 'C3', 'Pub', 'C1', 'FB', 'FB', 'C1', 'C2', 'C3', 'Pub', 'C2', 'C3', 'Pub', 'C2', 'C3', 'Pub', 'C1', 'C2', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'FB', 'FB', 'FB', 'FB', 'C1', 'C2', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'C2', 'C3', 'Pub', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'C2', 'C3', 'Pub', 'C2', 'Sleep']\n",
      "['C1', 'FB', 'C1', 'C2', 'Sleep']\n",
      "['C1', 'C2', 'C3', 'Pass', 'Sleep']\n",
      "['C1', 'FB', 'FB', 'FB', 'C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'C2', 'C3', 'Pass', 'Sleep']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    main_markov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see after probing the markov process that students are enclined to pass an awful amount of time on facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Reward Process\n",
    "\n",
    "A Markov Reward Process is a tuple $(\\mathcal S ,\\mathcal P,\\mathcal R ,\\gamma)$ where:\n",
    "* $ \\mathcal S $ a finite set of states\n",
    "* $ \\mathcal P $ a state transition probability matrix : $\\mathcal P _{ss'}= \\mathbb P [S_{t+1}=s'|S_{t}=s]$\n",
    "* $ \\mathcal R $ is a reward function, $ \\mathcal R _s = \\mathbb E [R_{t+1}|S_t = s] $ \n",
    "* $ \\gamma $ is a discount factor, $\\gamma \\in [0,1]$\n",
    "\n",
    "Here we present the Student reward process:\n",
    "\n",
    "<p align=\"center\">\n",
    "\t<img src=\"./Images/MRP.png\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StudentMarkovRewardProcess(StudentMarkovChain):\n",
    "    \"\"\"\n",
    "    Class to add rewards to the student markov chain\n",
    "    it is inheriting the transition probabilities and the names from the markov chain\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor \n",
    "        \"\"\"\n",
    "\n",
    "        StudentMarkovChain.__init__(self)\n",
    "        # we are adding here the rewards of the different states\n",
    "        self.rewards=[-2,-2,-2,10,1,-1,0]\n",
    "        #and the shape of the history includes the rewards \n",
    "        self.history[-1]=(self.history[-1],self.rewards[self.state])\n",
    "\n",
    "    # change the step function of the markov chain to add the rewards    \n",
    "    def step(self):\n",
    "        state,finished = StudentMarkovChain.step(self)\n",
    "        reward = self.rewards[state]\n",
    "        self.history[-1]=(self.history[-1],reward)\n",
    "        return self.state,finished,reward\n",
    "\n",
    "    #function to restart\n",
    "    def reboot(self):\n",
    "        self.state = 0\n",
    "        self.history = [(self.titles[self.state],self.rewards[self.state])]\n",
    "        \n",
    "#function to run the markov chain\n",
    "def main_markov_reward():\n",
    "    finished = False\n",
    "    srp = StudentMarkovRewardProcess()\n",
    "    while not finished:\n",
    "        _,finished,_ = srp.step()\n",
    "    print(srp.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C1', -2), ('C2', -2), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "[('C1', -2), ('C2', -2), ('Sleep', 0)]\n",
      "[('C1', -2), ('C2', -2), ('Sleep', 0)]\n",
      "[('C1', -2), ('C2', -2), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "[('C1', -2), ('C2', -2), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "[('C1', -2), ('C2', -2), ('C3', -2), ('Pub', 1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('C2', -2), ('C3', -2), ('Pub', 1), ('C3', -2), ('Pub', 1), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "[('C1', -2), ('FB', -1), ('C1', -2), ('C2', -2), ('C3', -2), ('Pub', 1), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "[('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('C2', -2), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "[('C1', -2), ('C2', -2), ('Sleep', 0)]\n",
      "[('C1', -2), ('C2', -2), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n"
     ]
    }
   ],
   "source": [
    "#the history now has a reward attached to it\n",
    "for i in range(10):\n",
    "    main_markov_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A central notion in reinforcement is the return $G_{t}$ that the agent can expect from now on to the end of the episode:\n",
    "$$ G_{t}= R_{t+1}+\\gamma R_{t+2}+ ... = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}$$\n",
    "\n",
    "The state value is of a MRP is the expected return starting from state $s$:\n",
    "$$ v(s) = \\mathbb E [G_{t}|S_t = s]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_return_markov_reward():\n",
    "    gamma =0.9\n",
    "    finished = False\n",
    "    srp = StudentMarkovRewardProcess()\n",
    "    while not finished:\n",
    "        _,finished,_ = srp.step()\n",
    "    print(srp.history)\n",
    "    print(\"Return of state 1: \",sum([gamma**j*reward for j,(_,reward) in enumerate(srp.history)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C1', -2), ('FB', -1), ('C1', -2), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('FB', -1), ('C1', -2), ('C2', -2), ('C3', -2), ('Pass', 10), ('Sleep', 0)]\n",
      "Return of state 1:  -8.45756140197053\n"
     ]
    }
   ],
   "source": [
    "compute_return_markov_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
