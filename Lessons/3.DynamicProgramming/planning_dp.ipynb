{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning by Dynamic programming\n",
    "I spent less time coding on this part of the lesson but it introduces key concepts such as Policy evaluation, Value Iteration... that will be used in the next lesson, so it is useful to know what is at stake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic programming assumes full knowledge of the MDP, this is a rather strong condition as we don't necesseraly know the transition probability or the reward that the agent will obtain by acting in a specific state: this allows us to assess the value function without running episodes\n",
    "\n",
    "I take the exemple of the previous class to illustrate the concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class StudentMarkovChain():\n",
    "    \"\"\"\n",
    "    This class models the Markov process described in the lesson\n",
    "    everything is hard coded, this class is not meant to be general\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #transition probabilities of each state\n",
    "        self.transition = np.array([[0, 0.5 , 0 , 0 , 0 , 0.5 , 0 ],\n",
    "                                    [0 , 0 , 0.8 , 0 , 0 , 0 , 0.2],\n",
    "                                    [0 , 0 , 0 , 0.6 , 0.4 , 0 , 0],\n",
    "                                    [0 , 0 , 0 , 0 , 0 , 0 , 1],\n",
    "                                    [0.2 , 0.4 , 0.4 , 0 , 0 , 0 , 0],\n",
    "                                    [0.1 , 0 , 0 , 0 , 0 , 0.9 , 0],\n",
    "                                    [0 , 0 , 0 , 0 , 0 , 0 , 1]\n",
    "                                    ])\n",
    "        #name of states\n",
    "        self.titles=[\"C1\",\"C2\",\"C3\",\"Pass\",\"Pub\",\"FB\",\"Sleep\"]\n",
    "        #first state\n",
    "        self.state=0\n",
    "        #the class will keep the history\n",
    "        self.history = [self.titles[self.state]]\n",
    "\n",
    "    #function to change state in the markov process\n",
    "    def step(self):\n",
    "        #Next state is picked following the probabilities of the transition matrix \n",
    "        self.state = np.random.choice(range(7),p=self.transition[self.state])\n",
    "        self.history.append(self.titles[self.state])\n",
    "        #if the state is  final\n",
    "        if self.state != 6:\n",
    "            #we return a state a bool telling id it is finished and a null reward\n",
    "            return self.state,False,0\n",
    "        else:\n",
    "            return self.state,True,0\n",
    "    #function to restart\n",
    "    def reboot(self):\n",
    "        self.state = 0\n",
    "        self.history = [self.titles[self.state]]\n",
    "\n",
    "\n",
    "#function to run the reward process or \n",
    "def main_markov():\n",
    "    finished = False\n",
    "    smc = StudentMarkovRewardProcess()\n",
    "    while not finished:\n",
    "        _,finished,_ = smc.step()\n",
    "    print(smc.history)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class StudentMarkovRewardProcess(StudentMarkovChain):\n",
    "    \"\"\"\n",
    "    Class to add rewards to the student markov chain\n",
    "    it is inheriting the transition probabilities and the names from the markov chain\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "\n",
    "        StudentMarkovChain.__init__(self)\n",
    "        # we are adding here the rewards of the different states\n",
    "        self.rewards=[-2,-2,-2,10,1,-1,0]\n",
    "        #and the shape of the history includes the rewards \n",
    "        self.history[-1]=(self.history[-1],self.rewards[self.state])\n",
    "\n",
    "    # change the step function of the markov chain to add the rewards    \n",
    "    def step(self):\n",
    "        state,finished,_ = StudentMarkovChain.step(self)\n",
    "        reward = self.rewards[state]\n",
    "        self.history[-1]=(self.history[-1],reward)\n",
    "        return self.state,finished,reward\n",
    "\n",
    "    #function to restart\n",
    "    def reboot(self):\n",
    "        self.state = 0\n",
    "        self.history = [(self.titles[self.state],self.rewards[self.state])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the transition probability we can compute iteratively the value of the states in the markov reward process, Policy evaluation allows us to assess the value of a given policy on a MDP as an MDP with fixed policy can be interpreted as an MRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(nb):\n",
    "    mrp = StudentMarkovRewardProcess()\n",
    "    state_values = np.zeros(7)\n",
    "    gamma = 0.9\n",
    "    for _ in range(nb):\n",
    "        state_values = np.array(mrp.rewards) + gamma*np.dot(np.array(mrp.transition),state_values)\n",
    "    print(dict(zip(mrp.titles,state_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C1': -5.012728910014519, 'C2': 0.9426552976939075, 'C3': 4.087021246797093, 'Pass': 10.0, 'Pub': 1.9083923522141468, 'FB': -7.637608431059506, 'Sleep': 0.0}\n"
     ]
    }
   ],
   "source": [
    "nb_iterations = 2000\n",
    "policy_evaluation(nb_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the same values that the direct solving of the MRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using policy evaluation we can assess the quality of the states given a policy, knowing the states that have the best value we can act accordingly.\n",
    "We will perform here policy evaluation on the MDP with a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovStudentDecisionProcess():\n",
    "    \"\"\"\n",
    "    Markov Decision Process of the Student graph presented in the lesson\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "\n",
    "        #less states than before \n",
    "        self.titles=[\"C1\",\"C2\",\"C3\",\"FB\",\"Sleep\"]\n",
    "        self.state=0\n",
    "        #transition probabilites depend on the action taken by the agent\n",
    "        self.transition = [{\"Study\":[0,1,0,0,0],'Facebook':[0,0,0,1,0]},\n",
    "                           {'Study':[0,0,1,0,0],'Sleep':[0,0,0,0,1]},\n",
    "                           {'Study':[0,0,0,0,1],'Pub':[0.2,0.4,0.4,0,0]},\n",
    "                           {'Facebook':[0,0,0,1,0],\"Quit\":[1,0,0,0,0]},\n",
    "                           {\"Sleep\":[0,0,0,0,1]}\n",
    "                            ]\n",
    "        #the reward depends on the state and action taken by the agent\n",
    "        self.reward=[{\"Study\":-2,'Facebook':-1},\n",
    "                   {'Study':-2,'Sleep':0},\n",
    "                   {'Study':10,'Pub':1},\n",
    "                   {'Facebook':-1,\"Quit\":0},\n",
    "                   {\"Sleep\":0}]\n",
    "\n",
    "\n",
    "    #same step depends now of the action taken by the agent    \n",
    "    def step(self,action):\n",
    "        reward = self.reward[self.state][action]\n",
    "        self.state = np.random.choice(range(5),p=self.transition[self.state][action])\n",
    "        \n",
    "        \n",
    "        finished=(action == \"Sleep\")\n",
    "        return self.state,reward,finished\n",
    "    #no history stored here \n",
    "    def reboot(self):\n",
    "        self.state = 0\n",
    "\n",
    "\n",
    "class AgentRandom():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Random agent has no will it just pick random actions\n",
    "        \"\"\"\n",
    "        self.actions = [[\"Study\",'Facebook'],\n",
    "                       ['Study','Sleep'],\n",
    "                       ['Study','Pub'],\n",
    "                       ['Facebook',\"Quit\"],\n",
    "                       [\"Sleep\"]]\n",
    "\n",
    "    #selecting action randomly                   \n",
    "    def select_action(self,state):\n",
    "        return np.random.choice(self.actions[state])\n",
    "\n",
    "    #virtual update\n",
    "    def update(self,**kwargs):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(nb_iteration):\n",
    "    mdp = MarkovStudentDecisionProcess()\n",
    "    agent = AgentRandom()\n",
    "    state_values = np.zeros(5)\n",
    "    gamma = 0.9\n",
    "    # performing policy evaluation\n",
    "    for _ in range(nb_iteration):\n",
    "        for state in range(5):\n",
    "            actions = agent.actions[state]\n",
    "            state_values[state] = (1/len(actions))*np.sum([mdp.reward[state][action] + gamma*np.sum(mdp.transition[state][action]*state_values) for action in actions])\n",
    "    print(state_values)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.48447749  2.15815786  7.01812859 -2.1236634   0.        ]\n"
     ]
    }
   ],
   "source": [
    "policy_evaluation(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this evaluation we can iterate on the policy acting in a greedy manner, this is called Policy iteration and it is bound to converge to the optimal policy in a finite environement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will stop here the scripts on the lesson even if there are key concepts, to focus more in detail on the model free prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
