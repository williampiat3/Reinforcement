{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and Exploitation\n",
    "This lesson introduces the problem of multi-arms bandits: in front of you there are K arms, each one of them giving you a different average rewards. The goal is to maximize your cumulated rewards over a given number of actions (we have a limited budget of actions). The problem is not an easy one to solve as it requires to balance Exploration (ie testing new arms) and Exploitation (ie pulling the arm that gave us the biggest amount of rewards)\n",
    "\n",
    "In this case there are only action, not states:\n",
    "\n",
    "The action value function is defined by:\n",
    "$$\n",
    "Q(a) = \\mathbb E [r|a]\n",
    "$$\n",
    "\n",
    "There are different approaches:\n",
    "* A purely random one that gives us a baseline\n",
    "* An $\\epsilon$-greedy one that work well in practice\n",
    "* An Upper Confidence Bound approach that uses uncertainty over the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
