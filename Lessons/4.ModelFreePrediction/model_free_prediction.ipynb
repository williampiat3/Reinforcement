{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Free prediction\n",
    "The goal of this lesson is to be able to estimate the value of an unknown MDP with a given policy, in the previous lesson we assumed the full knowledge of the MDP which allowed us to use iterative methods for computing the value function without probing any episode.\n",
    "\n",
    "Here on the contrary we will no assume that we know the MDP and we will introduce methods for computing the value function and the action-value function\n",
    "\n",
    "We will use the Student MRP coded in the previous lessons to illustrate our points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class StudentMarkovChain():\n",
    "    \"\"\"\n",
    "    This class models the Markov process described in the lesson\n",
    "    everything is hard coded, this class is not meant to be general\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #transition probabilities of each state\n",
    "        self.transition = np.array([[0, 0.5 , 0 , 0 , 0 , 0.5 , 0 ],\n",
    "                                    [0 , 0 , 0.8 , 0 , 0 , 0 , 0.2],\n",
    "                                    [0 , 0 , 0 , 0.6 , 0.4 , 0 , 0],\n",
    "                                    [0 , 0 , 0 , 0 , 0 , 0 , 1],\n",
    "                                    [0.2 , 0.4 , 0.4 , 0 , 0 , 0 , 0],\n",
    "                                    [0.1 , 0 , 0 , 0 , 0 , 0.9 , 0],\n",
    "                                    [0 , 0 , 0 , 0 , 0 , 0 , 1]\n",
    "                                    ])\n",
    "        #name of states\n",
    "        self.titles=[\"C1\",\"C2\",\"C3\",\"Pass\",\"Pub\",\"FB\",\"Sleep\"]\n",
    "        #first state\n",
    "        self.state=0\n",
    "        #the class will keep the history\n",
    "        self.history = [self.titles[self.state]]\n",
    "\n",
    "    #function to change state in the markov process\n",
    "    def step(self):\n",
    "        #Next state is picked following the probabilities of the transition matrix \n",
    "        self.state = np.random.choice(range(7),p=self.transition[self.state])\n",
    "        self.history.append(self.titles[self.state])\n",
    "        #if the state is  final\n",
    "        if self.state != 6:\n",
    "            #we return a state a bool telling id it is finished and a null reward\n",
    "            return self.state,False,0\n",
    "        else:\n",
    "            return self.state,True,0\n",
    "    #function to restart\n",
    "    def reboot(self):\n",
    "        self.state = 0\n",
    "        self.history = [self.titles[self.state]]\n",
    "\n",
    "\n",
    "#function to run the reward process or \n",
    "def main_markov():\n",
    "    finished = False\n",
    "    smc = StudentMarkovRewardProcess()\n",
    "    while not finished:\n",
    "        _,finished,_ = smc.step()\n",
    "    print(smc.history)\n",
    "\n",
    "class StudentMarkovRewardProcess(StudentMarkovChain):\n",
    "    \"\"\"\n",
    "    Class to add rewards to the student markov chain\n",
    "    it is inheriting the transition probabilities and the names from the markov chain\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "\n",
    "        StudentMarkovChain.__init__(self)\n",
    "        # we are adding here the rewards of the different states\n",
    "        self.rewards=[-2,-2,-2,10,1,-1,0]\n",
    "        #and the shape of the history includes the rewards \n",
    "        self.history[-1]=(self.history[-1],self.rewards[self.state])\n",
    "\n",
    "    # change the step function of the markov chain to add the rewards    \n",
    "    def step(self):\n",
    "        state,finished,_ = StudentMarkovChain.step(self)\n",
    "        reward = self.rewards[state]\n",
    "        self.history[-1]=(self.history[-1],reward)\n",
    "        return self.state,finished,reward\n",
    "\n",
    "    #function to restart\n",
    "    def reboot(self):\n",
    "        self.state = 0\n",
    "        self.history = [(self.titles[self.state],self.rewards[self.state])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo evaluation\n",
    "Here we run many episode and we compute the average of the return over all the episode for each state:\n",
    "$$ v_{\\pi}(s) = \\mathbb E _{\\pi} [G_t |S_t = s]$$\n",
    "\n",
    "Here is an implementation of the **every-visit** monte carlo policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MC eveluation of the return to compute the value function\n",
    "def monte_carlo_eval(number_of_sample):\n",
    "    finished = False\n",
    "    #initialize markov reward process\n",
    "    smc = StudentMarkovRewardProcess()\n",
    "    #values of the states\n",
    "    values = np.zeros(7)\n",
    "    #number of times each state was visited\n",
    "    numbers = np.zeros(7)\n",
    "    gamma = 0.9\n",
    "    #Here we start sampling episodes\n",
    "    for sample in tqdm(range(number_of_sample)):\n",
    "        finished=False\n",
    "        history=[(0,-2)]\n",
    "        #making a full evaluation of the process\n",
    "        while not finished:\n",
    "            state, finished, reward = smc.step()\n",
    "            history.append((state,reward))\n",
    "        #computing returns (sum discounted rewards)\n",
    "        returns=[]\n",
    "        #offline updates\n",
    "        for i in range(len(history)):\n",
    "            state = history[i][0]\n",
    "            current_return = sum([gamma**j*reward for j,(_,reward) in enumerate(history[i:])])\n",
    "            returns.append((state,current_return))\n",
    "        #updating values\n",
    "        for state,local_return in returns:\n",
    "            numbers[state]+=1\n",
    "            values[state]+=(local_return-values[state])/numbers[state]\n",
    "        smc.reboot()\n",
    "    print(dict(zip(smc.titles,values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:03<00:00, 524.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C1': -4.893740722266557, 'C2': 0.8574090139837567, 'C3': 4.028911854387622, 'Pass': 10.0, 'Pub': 2.002390065808052, 'FB': -7.520075693235476, 'Sleep': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "monte_carlo_eval(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we managed to compute the value of the states, without using an knowledge on the MRP. However Monte carlo doesn't allow to learn on the flight (inside an episode). All the updates are made offline (meaning after the episode was performed) The update that I will present afterwards will allow on-line updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to use the Markov property of the MDP (or MRP) to compute the state values and it relies on the Bellman equation\n",
    "\n",
    "$$ v_{\\pi}(s) = \\mathbb E _{\\pi} [R_{t+1} + \\gamma v_{\\pi}(S{t+1}) |S_t = s]$$\n",
    "\n",
    "Therefore we can rewritte the update of the state in the following manner:\n",
    "\n",
    "$$ V(S_t) \\rightarrow V(S_t) + \\alpha( R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))$$\n",
    "\n",
    "To perform the update we only need two consecutive states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TD(0) implemented for the student reward process\n",
    "def temporal_difference_eval(number_of_sample):\n",
    "    finished = False\n",
    "    smc = StudentMarkovRewardProcess()\n",
    "    values = np.zeros(7)\n",
    "    numbers = np.zeros(7)\n",
    "    gamma = 0.9\n",
    "    for sample in tqdm(range(number_of_sample)):\n",
    "        finished=False\n",
    "        state=smc.state\n",
    "        reward = smc.rewards[state]\n",
    "        #making a full evaluation of the process\n",
    "        while not finished:\n",
    "            former_state,former_reward = state,reward \n",
    "            state, finished, reward = smc.step()\n",
    "            #online updates\n",
    "            numbers[former_state]+=1\n",
    "            values[former_state] += (former_reward + gamma*values[state] -values[former_state])/numbers[former_state]\n",
    "        smc.reboot()\n",
    "    print(dict(zip(smc.titles,values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:07<00:00, 529.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C1': -4.251866548264395, 'C2': 1.0092612149890148, 'C3': 4.1734589428482005, 'Pass': 10.0, 'Pub': 2.1862308492278344, 'FB': -6.238723778685403, 'Sleep': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "temporal_difference_eval(4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to reduce the variance by introducing some bias in the TD target. But we would like to make the best of both worlds. A compromise can be found by using TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD($\\lambda$)\n",
    "\n",
    "We can obtain multiple TD targets by applying iteratively the bellman equation\n",
    "$$ G_t^{(1)} =  R_{t+1} + \\gamma V(S_{t+1} $$\n",
    "$$  G_t^{(2)} =  R_{t+1} + \\gamma  R_{t+2} + \\gamma^{2} V(S_{t+2}$$ \n",
    "$$ ... $$\n",
    "$$ G_t^{(n)}= R_{t+1} + \\gamma  R_{t+2} + ... + \\gamma^{n}R_{t+n}$$\n",
    "\n",
    "These target can now be averaged using a geometric law of parameter $\\lambda$\n",
    "$$ G_t^{\\lambda} = (1-\\lambda)\\sum_{n=1}^{\\infty}\\lambda^{n-1}G_t^{(n)} $$\n",
    "\n",
    "By making $\\lambda$ vary we can adjust whether we want the update to be closer to the Temporal difference update of the monte carlo update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TD lambda for the student reward process forward version\n",
    "def td_lambda(number_of_sample):\n",
    "    finished = False\n",
    "    smc = StudentMarkovRewardProcess()\n",
    "    values = np.zeros(7)\n",
    "    numbers = np.zeros(7)\n",
    "    gamma = 0.9\n",
    "    # parameter lambda for the update\n",
    "    lambda_td = 0.5\n",
    "    for sample in tqdm(range(number_of_sample)):\n",
    "        finished=False\n",
    "        state=smc.state\n",
    "        reward = smc.rewards[state]\n",
    "        #making a full evaluation of the process\n",
    "        history=[(0,-2)]\n",
    "        while not finished:\n",
    "\n",
    "            state, finished, reward = smc.step()\n",
    "            history.append((state,reward))\n",
    "\n",
    "\n",
    "        #offline updates (forward view)\n",
    "        returns = []\n",
    "        for i in range(len(history)):\n",
    "            state = history[i][0]\n",
    "            discounted_rewards = [reward*gamma**j for j,(_,reward) in enumerate(history[i:])]\n",
    "            tds = [sum(discounted_rewards[:j])+gamma**j*values[state] for j,(state,reward) in enumerate(history[i:])][1:]\n",
    "            if state != 6:\n",
    "                #renormalization of the lambda targets (pay attention to the 1- lambda**n) as it is characteritic of a finite reward process\n",
    "                td_target = (1-lambda_td)/(1-lambda_td**len(tds))*sum([td*lambda_td**j for j,td in enumerate(tds)])\n",
    "            else:\n",
    "                td_target=0\n",
    "            returns.append((state,td_target))\n",
    "\n",
    "        for state,td_target in returns:\n",
    "            numbers[state]+=1\n",
    "            values[state]+=(td_target-values[state])/numbers[state]\n",
    "\n",
    "        smc.reboot()\n",
    "    print(dict(zip(smc.titles,values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:06<00:00, 303.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C1': -4.413332452197901, 'C2': 0.7623870306962792, 'C3': 3.9575381417084747, 'Pass': 10.0, 'Pub': 1.8784545599088731, 'FB': -6.35082787121543, 'Sleep': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "td_lambda(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented here the forward view of TD($\\lambda$), the backward view can allow online updates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
