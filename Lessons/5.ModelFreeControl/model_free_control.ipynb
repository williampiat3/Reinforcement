{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Free Control\n",
    "After seeing in the last lesson the manners of evaluating a given policy we will now try to improve the policy gradualy so as to find the best one. \n",
    "\n",
    "As we are planing to \"act\" without a model of the MDP, selecting the best action can't be done by the value function on its own as we need the transition probability to compute the following max:\n",
    "$$ \\pi'(s) = \\text{argmax}_{a \\in \\mathcal A}(\\mathcal R _s^a + \\mathcal P _{ss'}^a V(s') $$\n",
    "\n",
    "However acting greedly with respect to the **action value function** is doable without the knowledge of the MDP:\n",
    "\n",
    "$$ \\pi'(s) = \\text{argmax}_{a \\in \\mathcal A}Q(s,a)$$\n",
    "\n",
    "Model Free control is therefore based on the action value function only\n",
    "\n",
    "In our discrete MDP, for controling, our agents we now store and compute return values for states and actions: it gives the famous Q table that contains the Q values for all the state for all the possible actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC or TD control\n",
    "\n",
    "The update of the Q value in the Q table can be done by doing either MC or TD approximation of the TD target for the action value function: \n",
    "If you are using a TD(0) or TD($\\lambda$) the algorithm is called SARSA or SARSA($\\lambda$)\n",
    "\n",
    "Using an approximate of the Q table we can act greedily with respect to it so as to improve our average return, to balance exploration and exploitation we improve the policy by acting in a $\\epsilon$-greedy manner with a decaying epsilon\n",
    "\n",
    "We illustrate the Q learning algorithm on the Student MDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MarkovStudentDecisionProcess():\n",
    "    \"\"\"\n",
    "    Markov Decision Process of the Student graph presented in the lesson\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "\n",
    "        #less states than before \n",
    "        self.titles=[\"C1\",\"C2\",\"C3\",\"FB\",\"Sleep\"]\n",
    "        self.state=0\n",
    "        #transition probabilites depend on the action taken by the agent\n",
    "        self.transition = [{\"Study\":[0,1,0,0,0],'Facebook':[0,0,0,1,0]},\n",
    "                           {'Study':[0,0,1,0,0],'Sleep':[0,0,0,0,1]},\n",
    "                           {'Study':[0,0,0,0,1],'Pub':[0.2,0.4,0.4,0,0]},\n",
    "                           {'Facebook':[0,0,0,1,0],\"Quit\":[1,0,0,0,0]},\n",
    "                           {\"Sleep\":[0,0,0,0,1]}\n",
    "                            ]\n",
    "        #the reward depends on the state and action taken by the agent\n",
    "        self.reward=[{\"Study\":-2,'Facebook':-1},\n",
    "                   {'Study':-2,'Sleep':0},\n",
    "                   {'Study':10,'Pub':1},\n",
    "                   {'Facebook':-1,\"Quit\":0},\n",
    "                   {\"Sleep\":0}]\n",
    "\n",
    "\n",
    "    #same step depends now of the action taken by the agent    \n",
    "    def step(self,action):\n",
    "        reward = self.reward[self.state][action]\n",
    "        self.state = np.random.choice(range(5),p=self.transition[self.state][action])\n",
    "        \n",
    "        \n",
    "        finished=(action == \"Sleep\")\n",
    "        return self.state,reward,finished\n",
    "    #no history stored here \n",
    "    def reboot(self):\n",
    "        self.state = 0\n",
    "\n",
    "class AgentRandom():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Random agent has no will it just pick random actions\n",
    "        \"\"\"\n",
    "        self.actions = [[\"Study\",'Facebook'],\n",
    "                       ['Study','Sleep'],\n",
    "                       ['Study','Pub'],\n",
    "                       ['Facebook',\"Quit\"],\n",
    "                       [\"Sleep\"]]\n",
    "\n",
    "    #selecting action randomly                   \n",
    "    def select_action(self,state):\n",
    "        return np.random.choice(self.actions[state])\n",
    "\n",
    "    #virtual update\n",
    "    def update(self,**kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class QlearnerAgent(AgentRandom):\n",
    "    \"\"\"\n",
    "    Epsilon Greedy agent using a Q table (in the form of a dictonnary as the possible actions depend of the state\n",
    "    it is inheriting form the random agent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,epsilon,gamma):\n",
    "        AgentRandom.__init__(self)\n",
    "        #Q table in the form of a dicionnary\n",
    "        self.q_table=[dict([(action,0) for action in actions]) for actions in self.actions]\n",
    "        #epsilon for the percentage of random actions\n",
    "        self.epsilon=epsilon\n",
    "        #discount\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def select_action(self,state):\n",
    "        #selecting action in a epsilon greedy fashion\n",
    "        if random.random()<self.epsilon:\n",
    "            return AgentRandom.select_action(self,state)\n",
    "        else:\n",
    "            return max(self.q_table[state].items(),key=lambda x: x[1])[0]\n",
    "        \n",
    "        \n",
    "    def update(self,state,action,reward,new_state):\n",
    "        #Update using the Q learning algorithm\n",
    "        q_max=max(self.q_table[new_state].items(),key=lambda x: x[1])[1]\n",
    "        self.q_table[state][action] += 0.1*(reward + self.gamma*q_max-self.q_table[state][action])\n",
    "\n",
    "def run_multiple_episodes(number):\n",
    "    gamma=0.9\n",
    "    epsilon=0.9\n",
    "    environement = MarkovStudentDecisionProcess()\n",
    "    agent = QlearnerAgent(epsilon,gamma)\n",
    "    for trial in tqdm(range(number)):\n",
    "        finished=False\n",
    "        while not finished:\n",
    "            state = environement.state\n",
    "            action = agent.select_action(state)\n",
    "            new_state,reward,finished = environement.step(action)\n",
    "            agent.update(state,action,reward,new_state)\n",
    "        agent.epsilon = max(0.1,agent.epsilon-0.01)\n",
    "\n",
    "\n",
    "        environement.reboot()\n",
    "    print(dict(zip(environement.titles,agent.q_table)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:01<00:00, 2319.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C1': {'Study': 4.299999999999988, 'Facebook': 2.4829998227279715}, 'C2': {'Study': 6.999999999999991, 'Sleep': 0.0}, 'C3': {'Study': 9.999999999999993, 'Pub': 7.741687482301987}, 'FB': {'Facebook': 0.277818151707509, 'Quit': 3.8699999910769614}, 'Sleep': {'Sleep': 0.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_multiple_episodes(4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we test the agent with this Q table on the student process with a greedy policy we obtain a very serious student :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
